{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c755ca69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Embedding, Bidirectional, LSTM, Dropout, Dense\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import class_weight\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "class ArabicSentimentClassifier:\n",
    "    def __init__(self, max_len=100, embedding_dim=300):\n",
    "        self.max_len = max_len\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.tokenizer = Tokenizer()\n",
    "        self.model = None\n",
    "        self.embedding_matrix = None\n",
    "\n",
    "    def load_data(self, csv_path):\n",
    "        df = pd.read_csv(csv_path)\n",
    "        df['Type'] = pd.to_numeric(df['Type'], errors='coerce')\n",
    "        df.dropna(subset=['Type'], inplace=True)\n",
    "        self.texts = df['Comment'].astype(str).values\n",
    "        self.labels = df['Type'].values\n",
    "        return self.texts, self.labels\n",
    "\n",
    "    def preprocess(self):\n",
    "        # Tokenize and pad\n",
    "        self.tokenizer.fit_on_texts(self.texts)\n",
    "        sequences = self.tokenizer.texts_to_sequences(self.texts)\n",
    "        self.word_index = self.tokenizer.word_index\n",
    "        padded = pad_sequences(sequences, maxlen=self.max_len)\n",
    "        return padded\n",
    "\n",
    "    def load_fasttext(self, fasttext_path):\n",
    "        print(\"Loading FastText embeddings...\")\n",
    "        embeddings_index = {}\n",
    "        with open(fasttext_path, encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                values = line.rstrip().split(' ')\n",
    "                word = values[0]\n",
    "                coefs = np.asarray(values[1:], dtype='float32')\n",
    "                embeddings_index[word] = coefs\n",
    "\n",
    "        num_words = len(self.word_index) + 1\n",
    "        self.embedding_matrix = np.zeros((num_words, self.embedding_dim))\n",
    "        for word, i in self.word_index.items():\n",
    "            embedding_vector = embeddings_index.get(word)\n",
    "            if embedding_vector is not None:\n",
    "                self.embedding_matrix[i] = embedding_vector\n",
    "\n",
    "        return num_words\n",
    "\n",
    "    def build_model(self, num_words, trainable=True):\n",
    "        input_layer = Input(shape=(self.max_len,))\n",
    "        embedding_layer = Embedding(\n",
    "            input_dim=num_words,\n",
    "            output_dim=self.embedding_dim,\n",
    "            weights=[self.embedding_matrix],\n",
    "            input_length=self.max_len,\n",
    "            trainable=trainable\n",
    "        )(input_layer)\n",
    "\n",
    "        x = Bidirectional(LSTM(128, dropout=0.4, recurrent_dropout=0.3))(embedding_layer)\n",
    "        x = Dropout(0.5)(x)\n",
    "        output = Dense(3, activation='softmax')(x)  # Changed here\n",
    "\n",
    "        self.model = Model(inputs=input_layer, outputs=output)\n",
    "        self.model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])  # Changed here\n",
    "        print(self.model.summary())\n",
    "\n",
    "\n",
    "    def train(self, X, y, val_split=0.1, batch_size=128, epochs=15):\n",
    "        early_stop = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True, verbose=1)\n",
    "\n",
    "        class_weights = class_weight.compute_class_weight(\n",
    "            class_weight='balanced',\n",
    "            classes=np.unique(y),\n",
    "            y=y\n",
    "        )\n",
    "        class_weights_dict = dict(enumerate(class_weights))\n",
    "\n",
    "        history = self.model.fit(\n",
    "            X, y,\n",
    "            validation_split=val_split,\n",
    "            batch_size=batch_size,\n",
    "            epochs=epochs,\n",
    "            callbacks=[early_stop],\n",
    "            class_weight=class_weights_dict\n",
    "        )\n",
    "\n",
    "        self.plot_history(history)\n",
    "\n",
    "    def cross_validate(self, X, y, folds=5, batch_size=128, epochs=10):\n",
    "        skf = StratifiedKFold(n_splits=folds, shuffle=True, random_state=20)\n",
    "        reports = []\n",
    "\n",
    "        fold_no = 1\n",
    "        for train_idx, val_idx in skf.split(X, y):\n",
    "            print(f\"\\nFold {fold_no}/{folds}\")\n",
    "\n",
    "            X_train, X_val = X[train_idx], X[val_idx]\n",
    "            y_train, y_val = y[train_idx], y[val_idx]\n",
    "\n",
    "            # Rebuild model for each fold\n",
    "            self.build_model(num_words=len(self.word_index) + 1, trainable=True)\n",
    "\n",
    "            early_stop = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True, verbose=0)\n",
    "\n",
    "            class_weights = class_weight.compute_class_weight(\n",
    "                class_weight='balanced',\n",
    "                classes=np.unique(y_train),\n",
    "                y=y_train\n",
    "            )\n",
    "            class_weights_dict = dict(enumerate(class_weights))\n",
    "\n",
    "            history = self.model.fit(\n",
    "                X_train, y_train,\n",
    "                validation_data=(X_val, y_val),\n",
    "                epochs=epochs,\n",
    "                batch_size=batch_size,\n",
    "                callbacks=[early_stop],\n",
    "                class_weight=class_weights_dict,\n",
    "                verbose=0\n",
    "            )\n",
    "\n",
    "            # Predict and report\n",
    "            y_pred_probs = self.model.predict(X_val) # get predicted probabilities\n",
    "            y_pred = np.argmax(y_pred_probs, axis=1) # convert probabilities to class labels\n",
    "            report = classification_report(y_val, y_pred, output_dict=True, zero_division=0)\n",
    "            reports.append(report)\n",
    "\n",
    "            print(classification_report(y_val, y_pred, zero_division=0))\n",
    "            fold_no += 1\n",
    "\n",
    "        return reports\n",
    "\n",
    "\n",
    "classifier = ArabicSentimentClassifier(max_len=100, embedding_dim=300)\n",
    "texts, labels = classifier.load_data('dataset.csv')\n",
    "X = classifier.preprocess()\n",
    "y = np.array(labels)\n",
    "num_words = classifier.load_fasttext('/content/cc.ar.300.vec')\n",
    "reports = classifier.cross_validate(X, y, folds=5, batch_size=128, epochs=15)\n",
    "f1_scores = [r['1.0']['f1-score'] if '1.0' in r else 0 for r in reports]\n",
    "y_pred = classifier.model.predict(X_val)\n",
    "y_pred = np.argmax(y_pred, axis=1)\n",
    "\n",
    "print(f\"\\nMaximum F1-score (toxic): {np.max(f1_scores):.4f}\")\n",
    "print(f\"\\nMaximum Accuracy (toxic): {np.max(f1_scores):.4f}\")\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_val, y_pred)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"Non-Toxic\", \"Toxic\"])\n",
    "disp.plot(cmap='Blues')\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.grid(False)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
